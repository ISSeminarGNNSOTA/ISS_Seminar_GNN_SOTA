# -*- coding: utf-8 -*-
"""XGB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Do43BWfoeT4rjs_gw2mihD5knOKFxGoG
"""

import xgboost as xgb
import numpy as np
import optuna
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split, KFold

class XGBModel:
    def __init__(self, ratings, objective='reg:squarederror'):
        self.ratings = ratings
        self.extract_features()
        # Default parameters - these can be overridden during hyperparameter tuning
        self.params = {
            'objective': objective,
            'colsample_bytree': 0.3,
            'learning_rate': 0.1,
            'max_depth': 5,
            'alpha': 10,
            'n_estimators': 100
        }

    def extract_features(self, ratings):
        X = np.column_stack((
        ratings['rating_count_per_user'],
        ratings['rating_count_per_movie'],
        ratings['avg_rating_per_person'],
        ratings['avg_rating_per_movie'],
        ratings['ReleaseAge'],
        # Add cluster features
        ratings['Cluster_0'], ratings['Cluster_1'], 
        ratings['Cluster_2'], ratings['Cluster_3'], 
        ratings['Cluster_4'],
        # Add user embedding features
        ratings['user_embedding_0'], ratings['user_embedding_1'],
        ratings['user_embedding_2'], ratings['user_embedding_3'],
        ratings['user_embedding_4'], ratings['user_embedding_5'],
        ratings['user_embedding_6'], ratings['user_embedding_7'],
        ratings['user_embedding_8'], ratings['user_embedding_9'],
        ratings['user_embedding_10'], ratings['user_embedding_11'],
        ratings['user_embedding_12'], ratings['user_embedding_13'],
        ratings['user_embedding_14'], ratings['user_embedding_15'],
        ratings['user_embedding_16'], ratings['user_embedding_17'],
        ratings['user_embedding_18'], ratings['user_embedding_19'],
        # Add movie embedding features
        ratings['movie_embedding_0'], ratings['movie_embedding_1'],
        ratings['movie_embedding_2'], ratings['movie_embedding_3'],
        ratings['movie_embedding_4'], ratings['movie_embedding_5'],
        ratings['movie_embedding_6'], ratings['movie_embedding_7'],
        ratings['movie_embedding_8'], ratings['movie_embedding_9'],
        ratings['movie_embedding_10'], ratings['movie_embedding_11'],
        ratings['movie_embedding_12'], ratings['movie_embedding_13'],
        ratings['movie_embedding_14'], ratings['movie_embedding_15'],
        ratings['movie_embedding_16'], ratings['movie_embedding_17'],
        ratings['movie_embedding_18'], ratings['movie_embedding_19']
        ))
    y = np.array(ratings['Rating'])
    return X, y

    

    def objective(self, trial):
        # Define the hyperparameter search space
        param = {
            'objective': self.params['objective'],
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'max_depth': trial.suggest_int('max_depth', 3, 20),
            'alpha': trial.suggest_float('alpha', 0, 100),
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        }

        # K-Fold Cross-Validation
        kf = KFold(n_splits=5, shuffle=True, random_state=888)
        rmse_scores = []

        for train_index, test_index in kf.split(self.X):
            X_train, X_test = self.X[train_index], self.X[test_index]
            y_train, y_test = self.y[train_index], self.y[test_index]

            model = xgb.XGBRegressor(**param)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            rmse = mean_squared_error(y_test, y_pred, squared=False)
            rmse_scores.append(rmse)

        return np.mean(rmse_scores)

    def tune_hyperparameters(self, n_trials=100):
        study = optuna.create_study(direction='minimize')
        study.optimize(self.objective, n_trials=n_trials)
        self.params.update(study.best_params)  # Update model parameters with best found
        return study.best_params


    def cross_validate(self, num_folds=5):
        start_time = time.time()  # Start timing for cross-validation
        kf = KFold(n_splits=num_folds, shuffle=True, random_state=888)
        rmse_scores, mae_scores, mse_scores = [], [], []

        for fold, (train_index, test_index) in enumerate(kf.split(self.X), start=1):
            X_train, X_test = self.X[train_index], self.X[test_index]
            y_train, y_test = self.y[train_index], self.y[test_index]

            model = xgb.XGBRegressor(**self.params)  # Use updated parameters
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            mse = mean_squared_error(y_test, y_pred)
            rmse = mse ** 0.5
            mae = mean_absolute_error(y_test, y_pred)

            mse_scores.append(mse)
            rmse_scores.append(rmse)
            mae_scores.append(mae)

            print(f"Fold {fold}/{num_folds}: MSE = {mse:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}")

        end_time = time.time()  # End timing for cross-validation
        total_time = end_time - start_time

        print(f"Average MSE: {np.mean(mse_scores):.4f}, Average RMSE: {np.mean(rmse_scores):.4f}, Average MAE: {np.mean(mae_scores):.4f}")
        print(f"Total time spent: {total_time:.2f} seconds")
        return np.mean(rmse_scores), np.mean(mae_scores)




    def predict(self, features):
        self.model = xgb.XGBRegressor(**self.params)  # Use updated parameters
        self.model.fit(self.X, self.y)
        return self.model.predict(features)
